labs(y = "Above 60", x = NULL)
cor.test(data = frequency2[frequency2$age == "Between 25 to 40",],
~ proportion + `Above 60`)
cor.test(data = frequency2[frequency2$age == "Between 40 to 60",],
~ proportion + `Above 60`)
frequency3 <- bind_rows(mutate(tidy_25to40, age = "Between 25 to 40"),
mutate(tidy_40to60, age ="Between 40 to 60"),
mutate(tidy_below25, age = "Below 25")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(age, word) %>%
group_by(age) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(age, proportion) %>%
gather(age, proportion, `Between 25 to 40`:`Between 40 to 60`)
ggplot(frequency3, aes(x = proportion, y = `Below 25`, color = abs(`Below 25` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~age, ncol = 3) +
theme(legend.position="none") +
labs(y = "Below 25", x = NULL)
cor.test(data = frequency3[frequency3$age == "Between 25 to 40",],
~ proportion + `Below 25`)
cor.test(data = frequency3[frequency3$age == "Between 40 to 60",],
~ proportion + `Below 25`)
frequency4 <- bind_rows(mutate(tidy_25to40, age = "Between 25 to 40"),
mutate(tidy_40to60, age ="Between 40 to 60")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(age, word) %>%
group_by(age) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(age, proportion) %>%
gather(age, proportion, `Between 25 to 40`)
ggplot(frequency4, aes(x = proportion, y = `Between 40 to 60`, color = abs(`Between 40 to 60` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~age, ncol = 3) +
theme(legend.position="none") +
labs(y = "Between 40 to 60", x = NULL)
cor.test(data = frequency4[frequency4$age == "Between 25 to 40",],
~ proportion + `Between 40 to 60`)
par(mfrow=c(1,5))
# Group1__age<25
text.1 <- gsub("[\\S]*http\\S+", " ", age_below25$cleaned_hm)
text.1 <- gsub("[^A-z'\"]", " ", text.1)
sentiment.values.1 <- get_sentiment(text.1, method="syuzhet")
summary(sentiment.values.1)
boxplot(sentiment.values.1, ylim=c(-5, 20), main="Age below 25", ylab="Sentiment of Moments")
#Group2__age(25,40)
text.2 <- gsub("[\\S]*http\\S+", " ", age_25to40$cleaned_hm)
text.2 <- gsub("[^A-z'\"]", " ", text.2)
sentiment.values.2 <- get_sentiment(text.2, method="syuzhet")
summary(sentiment.values.2)
boxplot(sentiment.values.2, ylim=c(-5, 20), main="Age betw 25to40", ylab="Sentiment of Moments")
#Group3__age(40,60)
text.3 <- gsub("[\\S]*http\\S+", " ", age_40to60$cleaned_hm)
text.3 <- gsub("[^A-z'\"]", " ", text.3)
sentiment.values.3 <- get_sentiment(text.3, method="syuzhet")
summary(sentiment.values.3)
boxplot(sentiment.values.3, ylim=c(-5, 20), main="Age betw 40to60", ylab="Sentiment of Moments")
#Group4__age above 60
text.4 <- gsub("[\\S]*http\\S+", " ", age_above60$cleaned_hm)
text.4 <- gsub("[^A-z'\"]", " ", text.4)
sentiment.values.4 <- get_sentiment(text.4, method="syuzhet")
summary(sentiment.values.4)
boxplot(sentiment.values.4, ylim=c(-5, 20), main="Age above 60", ylab="Sentiment of Moments")
frequency1 <- bind_rows(mutate(tidy_below25, age = "Below25"),
mutate(tidy_above60, age = "Above 60")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(age, word) %>%
group_by(age) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(age, proportion) %>%
gather(age, proportion, `Below25`)
ggplot(frequency1, aes(x = proportion, y = `Above 60`, color = abs(`Above 60` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "red75") +
facet_wrap(~age, ncol = 3) +
theme(legend.position="none") +
labs(y = "Above 60", x = NULL)
frequency1 <- bind_rows(mutate(tidy_below25, age = "Below25"),
mutate(tidy_above60, age = "Above 60")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(age, word) %>%
group_by(age) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(age, proportion) %>%
gather(age, proportion, `Below25`)
ggplot(frequency1, aes(x = proportion, y = `Above 60`, color = abs(`Above 60` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~age, ncol = 3) +
theme(legend.position="none") +
labs(y = "Above 60", x = NULL)
setwd("/Users/Anny/Documents/GitHub/Fall2018-Proj1-xf2168/output")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read.csv(urlfile,as.is=TRUE)
hm_data <- read.csv("../output/processed_moments.csv")
ageframe <- hm_data%>%
inner_join(demo_data, by = "wid") %>%
select(wid,
cleaned_hm,
age,
text)
ageframe$age= as.numeric(ageframe$age)
ageframe$text= as.character(ageframe$text)
ageframe<-ageframe[-which(is.na(ageframe$age)),]
?ggplot
## age below 25
tidy_below25 <- age_below25 %>%
unnest_tokens(word, text)
data(stop_words)
tidy_below25 <- tidy_below25 %>%
anti_join(stop_words)
tidy_below25 %>%
count(word, sort = TRUE) %>%
filter(n > 500) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n),color="red") +
geom_col() +
xlab(NULL) +
coord_flip()
## age below 25
tidy_below25 <- age_below25 %>%
unnest_tokens(word, text)
data(stop_words)
tidy_below25 <- tidy_below25 %>%
anti_join(stop_words)
tidy_below25 %>%
count(word, sort = TRUE) %>%
filter(n > 500) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(color="red") +
xlab(NULL) +
coord_flip()
tidy_above60 %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100,colors="red"))
## age below 25
tidy_below25 <- age_below25 %>%
unnest_tokens(word, text)
data(stop_words)
tidy_below25 <- tidy_below25 %>%
anti_join(stop_words)
tidy_below25 %>%
count(word, sort = TRUE) %>%
filter(n > 500) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(color="green") +
xlab(NULL) +
coord_flip()
## age between 25 to 40
tidy_25to40 <- age_25to40 %>%
unnest_tokens(word, text)
data(stop_words)
tidy_25to40 <- tidy_25to40 %>%
anti_join(stop_words)
tidy_25to40 %>%
count(word, sort = TRUE) %>%
filter(n > 1500) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(color="blue") +
xlab(NULL) +
coord_flip()
## age 40 to 60
tidy_40to60 <- age_40to60 %>%
unnest_tokens(word, text)
data(stop_words)
tidy_40to60 <- tidy_40to60 %>%
anti_join(stop_words)
tidy_40to60 %>%
count(word, sort = TRUE) %>%
filter(n > 250) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(color="yellow") +
xlab(NULL) +
coord_flip()
## age above 60
tidy_above60 <- age_above60 %>%
unnest_tokens(word, text)
data(stop_words)
tidy_above60 <- tidy_above60 %>%
anti_join(stop_words)
tidy_above60 %>%
count(word, sort = TRUE) %>%
filter(n > 50) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(color="red") +
xlab(NULL) +
coord_flip()
library(mallet)
text<-ageframe$text
# create a vector with one string per chapter
collapsed <- text %>%
anti_join(stop_words, by = "word") %>%
mutate(word = str_replace(word, "'", "")) %>%
group_by(document) %>%
summarize(text = paste(word, collapse = " "))
#load topic models library
library(topicmodels)
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 5
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
library(tm)
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
dtm <- DocumentTermMatrix(docs)
bigram_graph <- tidy_below25 %>%
filter(n > 1000) %>%
graph_from_data_frame()
bigram_count<-tidy_below25 %>%
count(word, sort = TRUE)
bigram_graph <- bigram_count %>%
filter(n > 1000) %>%
graph_from_data_frame()
install.packages("igraph")
library(igraph)
bigram_graph <- bigram_count %>%
filter(n > 1000) %>%
graph_from_data_frame()
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
install.packages(ggraph)
library(ggraph)
install.packages("ggraph")
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
library(igraph)
bigram_count<-tidy_below25 %>%
count(word, sort = TRUE)
bigram_graph <- bigram_count %>%
filter(n > 20) %>%
graph_from_data_frame()
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
library(igraph)
bigram_count<-tidy_below25 %>%
count(word, sort = TRUE)
bigram_graph <- bigram_count %>%
filter(n > 200) %>%
graph_from_data_frame()
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
text_word_pairs <- age_below25$cleaned_hm %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
install.packages("widyr")
library(widyr)
text_word_pairs <- age_below25$cleaned_hm %>%
pairwise_count(word, id, sort = TRUE, upper = FALSE)
word_counts<-tidy_40to60 %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
ungroup()
word_counts<-tidy_40to60 %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
ungroup()
word_dtm <- word_counts %>%
cast_dtm(id, word, n)
word_counts
word_counts<-tidy_40to60 %>%
anti_join(stop_words) %>%
count(id,word, sort = TRUE) %>%
ungroup()
word_counts<-tidy_40to60 %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
ungroup()
word_dtm <- word_counts %>%
cast_dtm(word, n)
word_counts<-tidy_40to60 %>%
anti_join(stop_words) %>%
count(wid,word, sort = TRUE) %>%
ungroup()
word_counts
word_dtm <- word_counts %>%
cast_dtm(wid,word, n)
word_counts<-tidy_40to60 %>%
anti_join(stop_words) %>%
count(id,word, sort = TRUE) %>%
ungroup()
below25_bigram<-age_below25 %>%
filter(count!=1)  %>%
unnest_tokens(bigram,text,token="ngrams",n=2)
#We get four groups of data according to different age range
# Group1__age<25
age_below25 <- ageframe%>%
filter(age <=25&age>7) %>%
select(wid, cleaned_hm,age,text,count)
#We use the processed data for our analysis and combine it with the demographic information available.Keep the required columns for analysis
ageframe <- hm_data%>%
inner_join(demo_data, by = "wid") %>%
select(wid,
cleaned_hm,
age,
text,count)
#We use the processed data for our analysis and combine it with the demographic information available.Keep the required columns for analysis
ageframe <- hm_data%>%
inner_join(demo_data, by = "wid") %>%
select(wid,
cleaned_hm,
age,
text,
count)
setwd("/Users/Anny/Documents/GitHub/Fall2018-Proj1-xf2168/output")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read.csv(urlfile,as.is=TRUE)
hm_data <- read.csv("../output/processed_moments.csv")
setwd("/Users/Anny/Documents/GitHub/Fall2018-Proj1-xf2168/output")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read.csv(urlfile,as.is=TRUE)
hm_data <- read.csv("../output/processed_moments.csv")
#We use the processed data for our analysis and combine it with the demographic information available.Keep the required columns for analysis
ageframe <- hm_data%>%
inner_join(demo_data, by = "wid") %>%
select(wid,
cleaned_hm,
age,
text,
count)
#We use the processed data for our analysis and combine it with the demographic information available.Keep the required columns for analysis
ageframe <- hm_data%>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
###Part0: Load libraries and text data; Separate into four groups
library(dplyr)
library(ngram)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(tidyr)
library(stringr)
library(scales)
library(syuzhet)
#We use the processed data for our analysis and combine it with the demographic information available.Keep the required columns for analysis
ageframe <- hm_data%>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
###Part0: Load libraries and text data; Separate into four groups
library(dplyr)
library(ngram)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(tidyr)
library(stringr)
library(scales)
library(syuzhet)
setwd("/Users/Anny/Documents/GitHub/Fall2018-Proj1-xf2168/output")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read.csv(urlfile,as.is=TRUE)
hm_data <- read.csv("../output/processed_moments.csv")
#We use the processed data for our analysis and combine it with the demographic information available.Keep the required columns for analysis
ageframe <- hm_data%>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
###Part0: Load libraries and text data; Separate into four groups
library(dplyr)
library(ngram)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(tidyr)
library(stringr)
library(scales)
library(syuzhet)
library(tidyverse)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
setwd("/Users/Anny/Documents/GitHub/Fall2018-Proj1-xf2168/output")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read.csv(urlfile,as.is=TRUE)
hm_data <- read.csv("../output/processed_moments.csv")
#We use the processed data for our analysis and combine it with the demographic information available.Keep the required columns for analysis
ageframe <- hm_data%>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
setwd("/Users/Anny/Documents/GitHub/Fall2018-Proj1-xf2168/output")
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read.csv(urlfile,as.is=TRUE)
hm_data <- read.csv("../output/processed_moments.csv")
hm_data
ageframe <- hm_data%>%
inner_join(demo_data, by = "wid") %>%
select(wid,
original_hm,
gender,
marital,
parenthood,
reflection_period,
age,
country,
ground_truth_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
library(ngram)
